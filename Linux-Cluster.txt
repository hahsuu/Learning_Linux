Cluster 
	系统扩展的方式
		scale up：向上扩展
		scale out：向外扩展

	集群类型
		LB，负载均衡集群  Load Balancing
		HA，高可用集群  High Availability
			Availability = 平均无故障时间 / (平均无故障时间 + 平均修改时间)
		HP，高性能集群  High Performancing

	系统：
		可扩展性
		可用性
		容量
		性能
	系统运维：可用 -> 标准化 -> 自动化

	构建高可扩展性系统的重要原则：在系统内部尽量避免串行和交差

	GSLB，Global Service Load Balancing
	SLB Service Load Balanceing

	总结：
		分层
		分割
		分布式
			分布式应用：分布式静态资源，分布式数据和存储，分布式计算

	LB集群的实现
		硬件
			F5 BIG-IP
			Critrix NetScaler
			A10 A10
			Atrag
			Redware
		软件
			lvs
			haproxy
			nginx
			ats(Apache Traffic Server)
			perlbal
		基于工作协议层划分
			传输层
				lvs  haproxy(mode tcp)
			应用层
				haproxy  niginx   ats   perlbal

	lvs, Linux Vitrual Server
		layer 4: 四层交换，四层路由
		根据请求报文的目标IP和PORT，将其转发至后端主机集群中的某一台主机(根据挑选算法)

		lvs: ipvsadm / ipvs
		ipvsadm：用户空间的命令工具，用于管理集群服务
		ipvs：工作于内核中的Netfilter INPUT钩子上
		支持TCP, UDP, AH, EST, AH_EST, SCTP等诸多协议

	lvs arch
		调试器：director, dispatcher, balancer
		rs：Real Server
		Client IP：CIP
		Director Virtual IP：VIP
		Director IP：DIP
		Real Server IP：RIP

	lvs type
		lvs-net  MASQUERADE
		lvs-dr(derict routing)  GAGEWAY
		lvs-tun(in tunneling)  IPIP
		lvs-fullnat

	lvs-net:
		多目标的DNAT，它通过修改请求报文的目标IP地址(必要时也可以修改端口)至挑选出某RS的RIP的地址实现转发
		(1)RS和DIP应使用私网地址，且RS的网关指向DIP
		(2)请求和响应的报文都要经过director转发，在极高负载的场景中，director可能会成为系统瓶颈
		(3)支持端口映射
		(4)RS可以使用任意OS
		(5)RS的RIP和DIP必须在同一IP网络

	lvs-dr  		Director：VIP，DIP  RSs：RIP，VIP
		它通过修改请求报文的目标MAC地址进行转发
		(1)保证前端路由器将目标IP为VIP的请求报文发给Director
			解决方案：静态绑定		arptables		修改RS主机内核参数(Linux)
		(2)RS的RIP可以使用私网地址，也可以是公网地址
		(3)RS跟Director必须在同一物理网络中
		(4)请求报文经由Director调度，但响应报文一定不能经由Director
		(5)不支持端口映射
		(6)RS可以是大多数OS
		(7)RS的网关不能指向DIP

	lvs-tun
		不修改请求报文的IP首部，而是通过在原有IP首部(CIP-VIP)之外，再封装一个IP首部(DIP-RIP)
		(1)RIP，DIP，VIP全是公网地址
		(2)RS的网关不能指向DIP
		(3)请求报文经由Director调度，但响应报文必须不能经由Director
		(4)不支持端口映射
		(5)RS的OS必须支持隧道功能

	lvs-fullnat
		director通过同时修改请求报文的目标地址和源地址进行转发
		(1)VIP是公网地址，RIP和DIP是私网地址，两者无须在同一网络
		(2)RS接收到的请求报文的源地址为DIP，因此要响应给DIP
		(3)请求响应报文必须经由Director
		(4)支持端口映射
		(5)RS可以使用任意OS

	http: stateless
		session保持
			session绑定：Source IP Hash			Cookie
			session集群，session在各个服务器之间进行复制，每台服务器都持有全部session
			session服务器

	lsv scheduler
		静态方法：仅根据算法本身进行调度
			RR：Round Robin，轮调
			WRR：Weighted RR
			SH：Source Hash，实现session保持，同一个iP被调度至同一个Server
			DH：Destination Hash，将对同一个目标请求始终发往同一个RS
		动态方法：根据算法及各RS的当前状态进行调度
			LC：Least Connection
				overhead = Active * 256 + Inactive
			WLC：Weighted LC
				overhead = (Active * 256 + Inactive) / weighted
			SED：Shortest Exception Delay
				overhead = (Active + 1) * 256 / weighted
			NQ：Never Queue
				SED算法的改进，保证每个RS先至少有一个连接
			LBLC：Locality-Based LC，动态DH算法，正向代理情形下的cache server调度
			LBLCR：Locality-Based LC whit Replication，带复制的LBLC算法

	ipvsadm的用法
		管理集群服务
		(1)一个ipvs主机可以同时定义多个Cluster Service
		(2)一个Cluster Service至少应该有一个Real Server
			ipvsadm -A|E -t|u|f service-address [-s scheduler] [-p [timeout]] [-M netmask]
			ipvsadm -D -t|u|f service-address

		管理集群服务中的RS
			ipvsadm -a|e -t|u|f service-address -r server-address [-g|i|m] [-w weight] [-x upper] [-y lower]
			ipvsadm -d -t|u|f service-address -r server-address
		清理和查看
			ipvsadm -C
			ipvsadm -L|l [options]
				-n numberic 不作名称反解，以数字形式显示
				-c --connection 显示连接
				--stats  显示统计数据
				--rate  显示速率
				--exact  显示匹配结果的精确值

		重载和保存
			ipvsadm -R
			ipvsadm -S [-n]
		
		置零计数器
			ipvsadm -Z [-t|u|f service-address]

		service-address
			tcp   -t ip:port
			upd   -u ip:port
			fwm   -f mark

		server-address
			ip[:port]

		lvs-type
			-g gateway dr
		 	-i ipip tun
		 	-m masquerade nat

		-s scheduler 默认为wlc

	lvs-nat

	lvs-dr
		两个内核参数
		arp-announce  是否主动通告					要设置为 2
		arp-igonre	  是否接收arp广播，并作出回应	要设置为 1

		Director：
			ifconfig eth0:0 172.16.100.10/32 broadcast 172.16.100.10 up
			route add -host 172.16.100.10 dev eth0:0

		RSs：
			echo 1 > /proc/sys/net/ipv4/conf/all/arp_ignore
			echo 1 > /proc/sys/net/ipv4/conf/eth0/arp_ignore
			echo 2 > /proc/sys/net/ipv4/conf/all/arp_announce
			echo 2 > /proc/sys/net/ipv4/conf/eth0/arp_announce

			ifconifg lo:0 172.16.100.10/32 broadcast 172.16.100.10 up
			route add -host 172.16.100.10 dev lo:0

	FWM：防火墙标记
		iptables -t mangle -A PREROUTING -d 192.168.0.10 -p tcp --dport 80 -j MARK --set-mark 10
		ipvsadm -A -f 10 -s rr
		ipvsadm -a -f 10 -r 172.16.100.21 -g

	通过FWM定义集群方式
		功能：将共享一组RS的集群服务统一进行定义

		1. Director上netfilter的mangle表的PREROUTING定义用于打标的规则
		iptables -t mangle -A PREROUTING -d $vip -p $protocol --dport $port - j MARK --set-mark #
		$vip  IP地址
		$protocol  协议
		$port  端口

		2. 基于FWM定义集群服务
		ipvsadm -A -f # -s scheduler
		ipvsadm -a -f # -r $rip

		

	lvs persistence   lvs持久连接
		功能：无论lvs采用何种算法，其都能实现将来自同一client的请求始终定向至第一次调度时挑选出来的RS
		待久连接模板：独立于算法
		source_ip		RS 		timer

		持久连接实现的方式：
			每端口持久：PPC，单服务持久调度
			每FWM持久：PFWMC，单FWM持久调度，port affinity
			每客户端持久：PCC，单客户端持久调度，Director会将用户所有请求识别为集群服务，将dport的值设置为0
				tcp  1-65536
				udp  1-65536

HA
	SPOF：Single Point of Failure
	directo：高可用集群
	real server：让Director对其做建康状态检测，并根据检测结果自动完成添加或移除等管理功能
	1. 基于协议层检查
		ip：icmp
		传输层：检测端口开放状态
		应用层：请求获取关键性资源

	2. 检查频度
	3. 状态判断
		下线：ok -> failure -> failure -> failure
		上线：failure -> ok -> ok -> ok

keepalived
	Linux Cluster
		LB: lvs, nginx
		HA: keepalived, heartbeat, corosync, cman
		HP:
			分布式存储：HDFS
			分布式计算：YARN
				batch：MapReduce
				in-memory：spark
				stream：storm

	keepavlived
		Active/passive
		lvs: vip, ipvs_rules
		nginx: vip, nginx_server
			resoure, 高可用资源

		VRRP: Virtual Route Redudent Protocol, 虚拟路由冗余协议
			master/backup

		keepalived，VRRP协议在Linux主机以守护进程方式实现
			能根据配置文件自动生成ipvs规则
			对各RS做健康状态检查
		组件：
			VRRP Stack
			chechers
			ipvs wrapper -> ipvs

		配置文件的组成部分
			GLOBAL CONFIGURATION
			VRRPD CONFIGURATION
				vrrp instance
				vrrp synchorization group
			LVS CONFIGURATION

		HA配置前提
			1. 主机名，本机的主机名与hosts定义的主机名保持一致，要与hostname(uname -n)获得的名称保持一致
			CentOS 6: /etc/sysconfig/network
			CentOS 7: hostanmectl set-hostname $HOSTNAME
			各节点要能互相解析主机名，一般建议通过hosts文件进行解析
			2. 各节点时间同步
			3. 确保iptables及selinux不会成为服务的阻碍

			virtual_ipaddress {
				172.16.100.80 dev eth0 label eth0:0
			}

			nopreempt  非抢占模式，默认为抢占模式

			vrrp_sync_group VG_1 {
				group {
					VI_1
					VI_2
				}
			}

		总结：每个vrrp_instance需要专用组播地址

		在VI中的主机状态发生改变时，发送通知

		示例：
			HTTP_GET {
				url {
					path /
					status_code 200
				}
				connect_timeout 3
				nb_get_retry 3
				delay_before_retry 3
			}

			TCP_CHECK {
				connect_timeout 3
			}





















