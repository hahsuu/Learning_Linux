Cluster 
	系统扩展的方式
		scale up：向上扩展
		scale out：向外扩展

	集群类型
		LB，负载均衡集群  Load Balancing
		HA，高可用集群  High Availability
			Availability = 平均无故障时间 / (平均无故障时间 + 平均修改时间)
		HP，高性能集群  High Performancing

	系统：
		可扩展性
		可用性
		容量
		性能
	系统运维：可用 -> 标准化 -> 自动化

	构建高可扩展性系统的重要原则：在系统内部尽量避免串行和交差

	GSLB，Global Service Load Balancing
	SLB Service Load Balanceing

	总结：
		分层
		分割
		分布式
			分布式应用：分布式静态资源，分布式数据和存储，分布式计算

	LB集群的实现
		硬件
			F5 BIG-IP
			Critrix NetScaler
			A10 A10
			Atrag
			Redware
		软件
			lvs
			haproxy
			nginx
			ats(Apache Traffic Server)
			perlbal
		基于工作协议层划分
			传输层
				lvs  haproxy(mode tcp)
			应用层
				haproxy  niginx   ats   perlbal

	lvs, Linux Vitrual Server
		layer 4: 四层交换，四层路由
		根据请求报文的目标IP和PORT，将其转发至后端主机集群中的某一台主机(根据挑选算法)

		lvs: ipvsadm / ipvs
		ipvsadm：用户空间的命令工具，用于管理集群服务
		ipvs：工作于内核中的Netfilter INPUT钩子上
		支持TCP, UDP, AH, EST, AH_EST, SCTP等诸多协议

	lvs arch
		调试器：director, dispatcher, balancer
		rs：Real Server
		Client IP：CIP
		Director Virtual IP：VIP
		Director IP：DIP
		Real Server IP：RIP

	lvs type
		lvs-net  MASQUERADE
		lvs-dr(derict routing)  GAGEWAY
		lvs-tun(in tunneling)  IPIP
		lvs-fullnat

	lvs-net:
		多目标的DNAT，它通过修改请求报文的目标IP地址(必要时也可以修改端口)至挑选出某RS的RIP的地址实现转发
		(1)RS和DIP应使用私网地址，且RS的网关指向DIP
		(2)请求和响应的报文都要经过director转发，在极高负载的场景中，director可能会成为系统瓶颈
		(3)支持端口映射
		(4)RS可以使用任意OS
		(5)RS的RIP和DIP必须在同一IP网络

	lvs-dr  		Director：VIP，DIP  RSs：RIP，VIP
		它通过修改请求报文的目标MAC地址进行转发
		(1)保证前端路由器将目标IP为VIP的请求报文发给Director
			解决方案：静态绑定		arptables		修改RS主机内核参数(Linux)
		(2)RS的RIP可以使用私网地址，也可以是公网地址
		(3)RS跟Director必须在同一物理网络中
		(4)请求报文经由Director调度，但响应报文一定不能经由Director
		(5)不支持端口映射
		(6)RS可以是大多数OS
		(7)RS的网关不能指向DIP

	lvs-tun
		不修改请求报文的IP首部，而是通过在原有IP首部(CIP-VIP)之外，再封装一个IP首部(DIP-RIP)
		(1)RIP，DIP，VIP全是公网地址
		(2)RS的网关不能指向DIP
		(3)请求报文经由Director调度，但响应报文必须不能经由Director
		(4)不支持端口映射
		(5)RS的OS必须支持隧道功能

	lvs-fullnat
		director通过同时修改请求报文的目标地址和源地址进行转发
		(1)VIP是公网地址，RIP和DIP是私网地址，两者无须在同一网络
		(2)RS接收到的请求报文的源地址为DIP，因此要响应给DIP
		(3)请求响应报文必须经由Director
		(4)支持端口映射
		(5)RS可以使用任意OS

	http: stateless
		session保持
			session绑定：Source IP Hash			Cookie
			session集群，session在各个服务器之间进行复制，每台服务器都持有全部session
			session服务器

	lsv scheduler
		静态方法：仅根据算法本身进行调度
			RR：Round Robin，轮调
			WRR：Weighted RR
			SH：Source Hash，实现session保持，同一个iP被调度至同一个Server
			DH：Destination Hash，将对同一个目标请求始终发往同一个RS
		动态方法：根据算法及各RS的当前状态进行调度
			LC：Least Connection
				overhead = Active * 256 + Inactive
			WLC：Weighted LC
				overhead = (Active * 256 + Inactive) / weighted
			SED：Shortest Exception Delay
				overhead = (Active + 1) * 256 / weighted
			NQ：Never Queue
				SED算法的改进，保证每个RS先至少有一个连接
			LBLC：Locality-Based LC，动态DH算法，正向代理情形下的cache server调度
			LBLCR：Locality-Based LC whit Replication，带复制的LBLC算法

	ipvsadm的用法
		管理集群服务
		(1)一个ipvs主机可以同时定义多个Cluster Service
		(2)一个Cluster Service至少应该有一个Real Server
			ipvsadm -A|E -t|u|f service-address [-s scheduler] [-p [timeout]] [-M netmask]
			ipvsadm -D -t|u|f service-address

		管理集群服务中的RS
			ipvsadm -a|e -t|u|f service-address -r server-address [-g|i|m] [-w weight] [-x upper] [-y lower]
			ipvsadm -d -t|u|f service-address -r server-address
		清理和查看
			ipvsadm -C
			ipvsadm -L|l [options]
				-n numberic 不作名称反解，以数字形式显示
				-c --connection 显示连接
				--stats  显示统计数据
				--rate  显示速率
				--exact  显示匹配结果的精确值

		重载和保存
			ipvsadm -R
			ipvsadm -S [-n]
		
		置零计数器
			ipvsadm -Z [-t|u|f service-address]

		service-address
			tcp   -t ip:port
			upd   -u ip:port
			fwm   -f mark

		server-address
			ip[:port]

		lvs-type
			-g gateway dr
		 	-i ipip tun
		 	-m masquerade nat

		-s scheduler 默认为wlc

	lvs-nat

	lvs-dr
		两个内核参数
		arp-announce  是否主动通告					要设置为 2
		arp-igonre	  是否接收arp广播，并作出回应	要设置为 1

		Director：
			ifconfig eth0:0 172.16.100.10/32 broadcast 172.16.100.10 up
			route add -host 172.16.100.10 dev eth0:0

		RSs：
			echo 1 > /proc/sys/net/ipv4/conf/all/arp_ignore
			echo 1 > /proc/sys/net/ipv4/conf/eth0/arp_ignore
			echo 2 > /proc/sys/net/ipv4/conf/all/arp_announce
			echo 2 > /proc/sys/net/ipv4/conf/eth0/arp_announce

			ifconifg lo:0 172.16.100.10/32 broadcast 172.16.100.10 up
			route add -host 172.16.100.10 dev lo:0

	FWM：防火墙标记
		iptables -t mangle -A PREROUTING -d 192.168.0.10 -p tcp --dport 80 -j MARK --set-mark 10
		ipvsadm -A -f 10 -s rr
		ipvsadm -a -f 10 -r 172.16.100.21 -g

	通过FWM定义集群方式
		功能：将共享一组RS的集群服务统一进行定义

		1. Director上netfilter的mangle表的PREROUTING定义用于打标的规则
		iptables -t mangle -A PREROUTING -d $vip -p $protocol --dport $port - j MARK --set-mark #
		$vip  IP地址
		$protocol  协议
		$port  端口

		2. 基于FWM定义集群服务
		ipvsadm -A -f # -s scheduler
		ipvsadm -a -f # -r $rip

		

	lvs persistence   lvs持久连接
		功能：无论lvs采用何种算法，其都能实现将来自同一client的请求始终定向至第一次调度时挑选出来的RS
		待久连接模板：独立于算法
		source_ip		RS 		timer

		持久连接实现的方式：
			每端口持久：PPC，单服务持久调度
			每FWM持久：PFWMC，单FWM持久调度，port affinity
			每客户端持久：PCC，单客户端持久调度，Director会将用户所有请求识别为集群服务，将dport的值设置为0
				tcp  1-65536
				udp  1-65536

HA
	SPOF：Single Point of Failure
	directo：高可用集群
	real server：让Director对其做建康状态检测，并根据检测结果自动完成添加或移除等管理功能
	1. 基于协议层检查
		ip：icmp
		传输层：检测端口开放状态
		应用层：请求获取关键性资源

	2. 检查频度
	3. 状态判断
		下线：ok -> failure -> failure -> failure
		上线：failure -> ok -> ok -> ok

keepalived
	Linux Cluster
		LB: lvs, nginx
		HA: keepalived, heartbeat, corosync, cman
		HP:
			分布式存储：HDFS
			分布式计算：YARN
				batch：MapReduce
				in-memory：spark
				stream：storm

	keepavlived
		Active/passive
		lvs: vip, ipvs_rules
		nginx: vip, nginx_server
			resoure, 高可用资源

		VRRP: Virtual Route Redudent Protocol, 虚拟路由冗余协议
			master/backup

		keepalived，VRRP协议在Linux主机以守护进程方式实现
			能根据配置文件自动生成ipvs规则
			对各RS做健康状态检查
		组件：
			VRRP Stack
			chechers
			ipvs wrapper -> ipvs

		配置文件的组成部分
			GLOBAL CONFIGURATION
			VRRPD CONFIGURATION
				vrrp instance
				vrrp synchorization group
			LVS CONFIGURATION

		HA配置前提
			1. 主机名，本机的主机名与hosts定义的主机名保持一致，要与hostname(uname -n)获得的名称保持一致
			CentOS 6: /etc/sysconfig/network
			CentOS 7: hostanmectl set-hostname $HOSTNAME
			各节点要能互相解析主机名，一般建议通过hosts文件进行解析
			2. 各节点时间同步
			3. 确保iptables及selinux不会成为服务的阻碍

			virtual_ipaddress {
				172.16.100.80 dev eth0 label eth0:0
			}

			nopreempt  非抢占模式，默认为抢占模式

			vrrp_sync_group VG_1 {
				group {
					VI_1
					VI_2
				}
			}

		总结：每个vrrp_instance需要专用组播地址

		在VI中的主机状态发生改变时，发送通知

		示例：
			HTTP_GET {
				url {
					path /
					status_code 200
				}
				connect_timeout 3
				nb_get_retry 3
				delay_before_retry 3
			}

			TCP_CHECK {
				connect_timeout 3
			}

Linux HA Cluster 
	HA Cluster
		AIS: heartbeat   OpenAIS: corosync/pacemaker    RHCS: cman/rgmanager(conga)

	故障场景
		硬件故障
			设计缺陷
			使用过久自然损坏
			人为故障
		软件故障
			设计缺陷
			bug
			人为误操作

	A = MTBF/(FTBF+MTTR)
	MTBF: Mean Time Between Failure
	MTTR: Mean Time To Repair


	network partition
		隔离：
		STONITH: Shoot The Other Node In The Head
		Fence: 资源级别隔离

	failover domain
	资源约束性
		位置约束：资源对节点的倾向性
		排列约束：资源彼此之间是否能运行于同一节点的倾向性
		顺序约束：多个资源启动顺序的依赖关系

	vote system
		少数服从多数：quorum
		超过总数的一半
		with quorum: 拥有法定票数
		without quorum: 不拥有法定票数

		两个节点需要仲裁设备
			ping node
			qdisk

		failover
		failback

	Messaging Layer:
		heartbeat v1, v2, v3  corosync  cman

	Cluster Resource Manager
		heartbeat v1  haresource(配置接口：配置文件, haresource)
		heartbeat v2  crm(在每个节点运行一个crmd守护进程，5560/tcp，有命令行接口，crmsh，GUI：heartbeat-gui, hb-gui)
		heartbeat v3  pacemaker(配置接口：crmsh, pcs, GUI: hawk(suse), LCMC, pacemaker-gui)
		rgmanager(配置接口：cluster.conf, system-config-cluster, conga(webgui))

	高可用集群的可用方案
		heartbeat v1 (haresource)
		heartbeat v2 (crm)
		heartbeat v3 + pacemaker
		corosync + pacemaker
			corosync v1 + pacemaker(plugin)
			corosync v2 + pacemaker(standalone service)
		cman + rgmanager
		cman + pacemaker
		corosync v1 + cman + pacemaker

		corosync v2 + pacemaker
		keepalived

		RHCS: Red Hat Cluster Suite
			RHEL5: cman + rgmanager + conga(ricci + luci)
			RHEL6: cman + rgmanager + conga(ricci + luci)
				   corosync + pacemaker
				   corosync + cman + pacemaker
			RHEL7: corosync + pacemaker

		配置：
			全局属性：property, stonith-enable等等
			高可用服务：资源，通过RA

	Resource Agent
		service
		LSB: /etc/rc.d/init.d/目录下的脚本
		OCF(Open Cluster Framework): [provider] 
			heartbeat
			pacemaker 
			linbit
		STONITH
		systemd  /etc/systemd/system/multi-user.target.wants
			处于enabled状态的服务

	资源类型
		primitive: 主资源，原始资源，在集群中只能运行一个实例
		clone: 克隆资源，在集群中可运行多个实例
			匿名克隆，全局惟一克隆，状态克隆(主动，被动)
		multi-state(master/slave): 克隆资源的特殊实现，多状态资源

	资源属性
		priority: 优先级
		target-role: started, stopped, master
		is-manageed: 是否允许集群管理资源
		resource-stickness: 资源粘性
		allow-migrate: 是否允许迁移

	资源约束 score
		位置约束：资源对节点的倾向性
			(-∞, +∞)
			任意值 + 无穷大 = 无穷大
			任意值 + 负无穷大 = 负无穷大
			无穷大 + 负无穷大 = 负无穷大
		排列约束：资源彼此之间是否能运行于同一节点的倾向性
			(-∞, +∞)
		顺序约束：多个资源启动顺序的依赖关系
			(-∞, +∞)
			mandatory，强制，相当于无穷大

	安装配置
		CentOS7: corosync v2 + pacemaker
			corosync v2 vote system
			pacemaker  独立服务

		集群生命周期管理工具
			pcs: agent  pcsd
			crmsh: agentless(pssh)

		配置集群前提
			1. 时间同步
			2. 基于当前在使用的主机名互相访问
			3. 是否会用到仲裁设备

	heartbeat信息传递
		unicast, udpu
		multicast, udp
		broadcast

		组播地址：用于标识一个IP组播域，IANA把D类地址留给组播使用，224.0.0.0 - 239.255.255.255
			永久组播地址：224.0.0.0 - 224.0.0.255
			临时组播地址：224.0.1.0 - 238.255.255.255
			本地组播地址：239.0.0.0 - 239.255.255.255

	HA Web Service
		vip: 172.16.100.92  ocf:heartbeat:IPaddr
		httpd: systemd
		nfs shared storage: ocf:heartbeat:Filesystem
	示例：

		totem {
			version: 2

			crypto_cipher: aes128
			crypto_hash: sha1
			setauth: on

			interface {
				ringnumber: 0
				bindnetaddr: 172.16.0.0
				mcastaddr: 239.185.1.31
				mcastport: 5405
				ttl: 1
			}
		}

		nodelist {
			node {
				ring0_addr: 172.16.100.67
				nodeid: 1
			}
			node {
				ring0_addr: 172.16.100.68
				nodeid: 2
			}
			node {
				ring0_addr: 172.16.100.69
				nodeid: 3
			}
		}

		logging {
			fileline: off
			to_stderr: no
			to_logfile: yes
			logfile: /var/log/cluster/corosync.log
			to_syslog: no
			timestamp: on
			logger_subsys {
				subsys: QUORUM
				debug: off
			}
		}

		quorum {
			provideer: corosync_votequorum
		}

	某些环境中可能不支持组播，这时应该配置 corosync 使用单播，下面是使用单播的 corosync 配置文件的一部分

		totem {
			#...
			interface {
				ringnummber: 0
				bindnetaddr: 192.168.42.0
				broadcast: yes
				mcastport: 5405
			}
			interface {
				ringnummber: 1
				bindnetaddr: 10.0.42.0
				broadcast: yes
				mcastport: 5405
			}
			transport: udpu
		}

		nodelist {
			node {
				ring0_addr: 192.168.42.1
				ring1_addr: 10.0.42.1
				nodeid: 1
			}
			node {
				ring0_addr: 192.168.42.2
				ring1_addr: 10.0.42.2
				nodeid: 2
		}

	HA Cluster工作模型
		A/P: 两节点集群  active/passive
			without-quorum-policy={stop | ignore | suicide | freeze}

		A/A: 

		webip, webstorage, webserver
			node1: 100 + 0 + 0
			node2: 0 + 0 + 0
			node3: 0 + 0 + 0

			node: 50 + 50 + 50 

	pcs:
		cluster
			auth
			setup
		resource
			describe
			list
			create
			delete
		constraint
			colocation
			order
			location
		property
			list
			set
		status
		config





















